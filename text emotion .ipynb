{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import preprocessing \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn \n",
    "from textblob import Word \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>showMe_Heaven</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>drapeaux</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>JenniRox</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>ipdaman1</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>Alpharalpha</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \n",
       "0      @tiffanylue i know  i was listenin to bad habi...  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "...                                                  ...  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('text_emotion.csv',encoding = \"ISO-8859-1\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('text_emotion.csv')\n",
    "\n",
    "data = data.drop('author', axis=1)\n",
    "\n",
    "data = data.drop(data[data.sentiment == 'anger'].index)\n",
    "data = data.drop(data[data.sentiment == 'boredom'].index)\n",
    "data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n",
    "data = data.drop(data[data.sentiment == 'empty'].index)\n",
    "data = data.drop(data[data.sentiment == 'fun'].index)\n",
    "data = data.drop(data[data.sentiment == 'relief'].index)\n",
    "data = data.drop(data[data.sentiment == 'surprise'].index)\n",
    "data = data.drop(data[data.sentiment == 'love'].index)\n",
    "data = data.drop(data[data.sentiment == 'hate'].index)\n",
    "data = data.drop(data[data.sentiment == 'neutral'].index)\n",
    "data = data.drop(data[data.sentiment == 'worry'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all letters lowercase\n",
    "data['content'] = data['content'].apply(\n",
    "    lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Removing Punctuation, Symbols\n",
    "data['content'] = data['content'].str.replace('[^\\w\\s]', ' ')\n",
    "\n",
    "# Removing Stop Words using NLTK\n",
    "stop = stopwords.words('english')\n",
    "data['content'] = data['content'].apply(\n",
    "    lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Lemmatisation\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(\n",
    "    [Word(word).lemmatize() for word in x.split()]))\n",
    "    \n",
    "# Correcting Letter Repetitions\n",
    "def de_repeat(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "    \n",
    "data['content'] = data['content'].apply(\n",
    "    lambda x: \" \".join(de_repeat(x) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "data['content'] = data['content'].apply(\n",
    "    lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data.content.values, y, stratify=y, random_state=42, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(data['content'])\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_val_count = count_vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvm using count vectors accuracy 0.7805974943784131\n"
     ]
    }
   ],
   "source": [
    "lsvm = SGDClassifier(alpha=0.01, random_state=5, max_iter=100, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalSentence = int(input('Total sentence to input: '))\n",
    "\n",
    "sentences = []\n",
    "for i in range(totalSentence):\n",
    "    sentences.append(input(f'Sentence {i + 1}: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0] = tweets[0].str.replace('[^\\w\\s]', ' ')\n",
    "stop = stopwords.words('english')\n",
    "tweets[0] = tweets[0].apply(lambda x: \" \".join(\n",
    "    x for x in x.split() if x not in stop))\n",
    "tweets[0] = tweets[0].apply(lambda x: \" \".join(\n",
    "    [Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "tweet_count = count_vect.transform(tweets[0])\n",
    "tweet_pred = lsvm.predict(tweet_count)\n",
    "\n",
    "print('\\nNo | Mood  | Sentence ')\n",
    "for i in range(totalSentence):\n",
    "    if (tweet_pred[i] == 0):\n",
    "        print(f'{i + 1}  | Happy | ', end='')\n",
    "    else:\n",
    "        print(f'{i + 1}  | Sad   | ', end='')\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
